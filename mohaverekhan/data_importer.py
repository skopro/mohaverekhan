import requests
import os
import pandas as pd

import multiprocessing
import json
import glob
from joblib import Parallel, delayed
import xml.etree.ElementTree as ET
import errno
import re
if __name__ == '__main__':
    import utils
else:
    from mohaverekhan import utils

logger = utils.get_logger(logger_name='data_importer')

base_api_url = r'http://127.0.0.1:8000/mohaverekhan/api'

words_url = fr'{base_api_url}/words'
word_normals_url = fr'{base_api_url}/word-normals'

texts_url = fr'{base_api_url}/texts'
text_normals_url = fr'{base_api_url}/text-normals'
text_tags_url = fr'{base_api_url}/text-tags'

tag_sets_url = fr'{base_api_url}/tag-sets'
tags_url = fr'{base_api_url}/tags'

tokens_url = fr'{base_api_url}/tokens'
token_tags_url = fr'{base_api_url}/token-tags'

validators_url = fr'{base_api_url}/validators'
normalizers_url = fr'{base_api_url}/normalizers'
taggers_url = fr'{base_api_url}/taggers'
# sentences_url = fr'{base_api_url}/sentences'
# normal_sentences_url = fr'{base_api_url}/normal-sentences'
# tagged_sentences_url = fr'{base_api_url}/tagged-sentences'

# translation_character_url = fr'{base_api_url}/rules/translation-characters'
# refinement_pattern_url = fr'{base_api_url}/rules/refinement-patterns'

data_dir= fr'/home/bitianist/Dropbox/bachelor_thesis/data'
text_equivalents_path = fr'{data_dir}/seq2seq/text_equivalents.xlsx'
word_equivalents_path = fr'{data_dir}/seq2seq/word_equivalents.xlsx'
bijankhan_data_dir = fr'{data_dir}/pos/bijankhan-online/unannotated'

bijankhan_tag_set_dictionary = [
  {
    "name": "E",
    "persian": "Ø­Ø±Ù Ø§Ø¶Ø§ÙÙ‡",
    "color": "#BCFF05"
  },
  {
    "name": "N",
    "persian": "Ø§Ø³Ù…",
    "color": "#FBFCFC"
  },
  {
    "name": "V",
    "persian": "ÙØ¹Ù„",
    "color": "#33B4FF"
  },
  {
    "name": "J",
    "persian": "Ø­Ø±Ù Ø±Ø¨Ø·",
    "color": "#1ABC9C"
  },
  {
    "name": "A",
    "persian": "ØµÙØª",
    "color": "#FFF82E"
  },
  {
    "name": "U",
    "persian": "Ø¹Ø¯Ø¯",
    "color": "#C7FFFB"
  },
  {
    "name": "T",
    "persian": "Ù‚ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø±",
    "color": "#BCCEF1"
  },
  {
    "name": "Z",
    "persian": "Ø¶Ù…ÛŒØ±",
    "color": "#FF82FF"
  },
  {
    "name": "O",
    "persian": "Ø¹Ù„Ø§Ù…Øª",
    "color": "#FFA14F"
  },
  {
    "name": "L",
    "persian": "ÙˆØ§Ø­Ø¯",
    "color": "#FF1F96"
  },
  {
    "name": "P",
    "persian": "Ø­Ø±Ù Ø§Ø¶Ø§ÙÙ‡ Ù¾Ø³ÛŒÙ†",
    "color": "#16DB00"
  },
  {
    "name": "D",
    "persian": "Ù‚ÛŒØ¯",
    "color": "#FF5442"
  },
  {
    "name": "C",
    "persian": "Ù…ØªØµÙ„â€ŒØ´ÙˆÙ†Ø¯Ù‡",
    "color": "#20EBC4"
  },
  {
    "name": "R",
    "persian": "R",
    "color": "#922B21"
  },
  {
    "name": "I",
    "persian": "Ø­Ø±Ù Ù†Ø¯Ø§",
    "color": "#AED6F1"
  }
]

bitianist_tag_set_dictionary = bijankhan_tag_set_dictionary + [
  {
    "name": "X",
    "persian": "Ø§ÛŒÙ…ÙˆØ¬ÛŒ",
    "color": "#00B3FF"
  },
  {
    "name": "S",
    "persian": "Ø´Ù†Ø§Ø³Ù‡",
    "color": "#00B3FF"
  },
  {
    "name": "K",
    "persian": "Ù„ÛŒÙ†Ú©",
    "color": "#00B3FF"
  },
  {
    "name": "M",
    "persian": "Ø§ÛŒÙ…ÛŒÙ„",
    "color": "#00B3FF"
  },
  {
    "name": "G",
    "persian": "ØªÚ¯",
    "color": "#00B3FF"
  },
]
def make_pretty_json_from_dictionary(dictionary):
    return json.dumps(dictionary, ensure_ascii=False, indent=4,)

def post(url, data_dictionary, log_it=False):
    data = json.dumps(data_dictionary)
    if log_it:
        logger.info(f'>> New request to {url} : {make_pretty_json_from_dictionary(data_dictionary)}')
    response = requests.post(url, data=data.encode('utf-8'), headers={'Content-type': 'application/json; charset=utf-8'})
    if response.status_code != 200 and response.status_code != 201:
        logger.info(f'> Error in request to \n{url}  \n\n{make_pretty_json_from_dictionary(data_dictionary)} \n\nError: \n\n{response.status_code} {response.text}\n\n')
        return response, True
    # if log_it:
    logger.info(f'> Success : {response.status_code} {response.text[:50]}...')
    return response, False

def put(url, data_dictionary, log_it=False):
    data = json.dumps(data_dictionary)
    if log_it:
        logger.info(f'>> New request to {url} : {make_pretty_json_from_dictionary(data_dictionary)}')
    response = requests.put(url, data=data.encode('utf-8'), headers={'Content-type': 'application/json; charset=utf-8'})
    if response.status_code != 200 and response.status_code != 201:
        logger.info(f'> Error in request to \n{url}  \n\n{make_pretty_json_from_dictionary(data_dictionary)} \n\nError: \n\n{response.status_code} {response.text}\n\n')
        return response, True
    # if log_it:
    logger.info(f'> Success : {response.status_code} {response.text[:50]}...')
    return response, False



def generate_normalizer_dictionary(name, show_name, is_automatic=False, 
                                owner=None, model_details=None, id=None):
    d = {}
    d['name'] = name
    d['show_name'] = show_name
    d['is_automatic'] = is_automatic
    if owner:
        d['owner'] = owner
    if model_details:
        d['model_details'] = model_details 
    if id:
        d['id'] = id 
    return d


def generate_tagger_dictionary(name, show_name, is_automatic=False, owner=None,
                             model_details=None, tag_set=None, id=None):
    d = {}
    d['name'] = name
    d['show_name'] = show_name
    d['is_automatic'] = is_automatic
    if owner:
        d['owner'] = owner
    if tag_set:
        d['tag_set'] = tag_set 
    if model_details:
        d['model_details'] = model_details 
    if id:
        d['id'] = id 
    return d



def generate_validator_dictionary(name, show_name, owner=None, id=None):
    d = {}
    d['name'] = name
    d['show_name'] = show_name
    if owner:
        d['owner'] = owner
    if id:
        d['id'] = id 
    return d




def generate_tag_set_dictionary(name, id=None, tags=None):
    d = {}
    d['name'] = name
    if id:
        d['id'] = id
    if tags:
        d['tags'] = tags
    return d



def generate_tag_dictionary(id=None, name=None, persian=None, color=None, tag_set=None):
    d = {}
    if id:
        d['id'] = id
    if name:
        d['name'] = name
    if persian:
        d['persian'] = persian
    if color:
        d['color'] = color
    if tag_set:
        d['tag_set'] = tag_set
    return d






def generate_text_dictionary(content, id=None):
    d = {}
    d['content'] = content
    if id:
        d['id'] = id 
    return d

def generate_text_normal_dictionary(content, normalizer, 
        text, id=None):
    d = {}
    d['content'] = content
    d['normalizer'] = normalizer
    d['text'] = text
    if id:
        d['id'] = id 
    return d

def generate_text_tag_dictionary(tagged_tokens, tagger, 
        text, id=None):
    d = {}
    d['tagged_tokens'] = tagged_tokens
    d['tagger'] = tagger
    d['text'] = text
    if id:
        d['id'] = id 
    return d





def generate_word_dictionary(content, id=None):
    d = {}
    d['content'] = content
    if id:
        d['id'] = id 
    return d

def generate_word_normal_dictionary(content, normalizer, 
        word, id=None):
    d = {}
    d['content'] = content
    d['normalizer'] = normalizer
    d['word'] = word
    if id:
        d['id'] = id 
    return d



def generate_tagged_token_dictionary(token, tag=None):
    d = {}
    d['token'] = token
    if tag:
        d['tag'] = tag    
    return d


@utils.time_usage(logger)
def import_tag_sets():
    # 0
    bijankhan_tag_set = generate_tag_set_dictionary('bijankhan-tag-set', 
        tags=bijankhan_tag_set_dictionary)

    response, error = post(tag_sets_url, bijankhan_tag_set)
    if error:
        return 0

    # 1
    bitianist_tag_set = generate_tag_set_dictionary('bitianist-tag-set', 
        tags=bitianist_tag_set_dictionary)

    response, error = post(tag_sets_url, bitianist_tag_set)
    if error:
        return 0

@utils.time_usage(logger)
def import_validators():
    bitianist_validator = generate_validator_dictionary(
        'bitianist-validator',
        show_name='Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ Ø¨ÛŒØªÛŒØ§Ù†ÛŒØ³Øª',
        owner='bitianist'
    )

    response, error = post(validators_url, bitianist_validator)
    if error:
        return 0

@utils.time_usage(logger)
def import_normalizers():
    # # 1
    # model_details = {
    #    'type': 'manual'
    # }
    # bitianist_bitianist_normalizer = generate_normalizer_dictionary(
    #     'bitianist-manual-normalizer',
    #     show_name='Ù†Ø±Ù…Ø§Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¯Ø³ØªÛŒ Ø¨ÛŒØªÛŒØ§Ù†ÛŒØ³Øª',
    #     owner='bitianist',
    #     is_automatic=False,
    #     model_details=model_details
    # )
    # response, error = post(normalizers_url, bitianist_bitianist_normalizer)
    # if error:
    #     return


    # # 2
    # model_details = {
    #     'type': 'rule-based',
    #     'state': 'ready'
    # }
    # bitianist_refinement_normalizer = generate_tagger_dictionary(
    #     'bitianist-refinement-normalizer',
    #     show_name='Ù†Ø±Ù…Ø§Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù¾Ø§Ù„Ø§ÛŒØ´',
    #     owner='bitianist',
    #     is_automatic=True,
    #     model_details=model_details
    # )
    # response, error = post(normalizers_url, bitianist_refinement_normalizer)
    # if error:
    #     return


    # # 3
    # model_details = {
    #     'type': 'stochastic',
    #     'module': 'seq2seq',
    #     'state': 'not-ready'
    # }
    # bitianist_seq2seq_normalizer = generate_tagger_dictionary(
    #     'bitianist-seq2seq-normalizer',
    #     show_name='Ù†Ø±Ù…Ø§Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ ØªÙˆØ§Ù„ÛŒâ€ŒØ¨Ù‡â€ŒØªÙˆØ§Ù„ÛŒ',
    #     owner='bitianist',
    #     is_automatic=True,
    #     model_details=model_details
    # )
    # response, error = post(normalizers_url, bitianist_seq2seq_normalizer)
    # if error:
    #     return

    # # 4
    # model_details = {
    #     'type': 'rule-based',
    #     'state': 'ready'
    # }
    # bitianist_replacement_normalizer = generate_tagger_dictionary(
    #     'bitianist-replacement-normalizer',
    #     show_name='Ù†Ø±Ù…Ø§Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ',
    #     owner='bitianist',
    #     is_automatic=True,
    #     model_details=model_details
    # )
    # response, error = post(normalizers_url, bitianist_replacement_normalizer)
    # if error:
    #     return

    # 5
    model_details = {
        'type': 'rule-based',
        'state': 'ready'
    }
    bitianist_basic_normalizer = generate_tagger_dictionary(
        'bitianist-basic-normalizer',
        show_name='Ù†Ø±Ù…Ø§Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ',
        owner='bitianist',
        is_automatic=True,
        model_details=model_details
    )
    response, error = post(normalizers_url, bitianist_basic_normalizer)
    if error:
        return
    

@utils.time_usage(logger)
def import_taggers():
    # # 0
    # model_details = {
    #    'type': 'manual'
    # }

    # bijankhan_manual_tagger = generate_tagger_dictionary(
    #     'bijankhan-manual-tagger',
    #     show_name='Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù† Ø¯Ø³ØªÛŒ Ø¨ÛŒâ€ŒØ¬Ù†â€ŒØ®Ø§Ù†',
    #     owner='bijankhan',
    #     tag_set='bijankhan-tag-set',
    #     is_automatic=False,
    #     model_details=model_details
    # )

    # response, error = post(taggers_url, bijankhan_manual_tagger)
    # if error:
    #     return 0



    # # 2
    # model_details = {
    #    'type': 'manual',
    #    'description': "For new words"
    # }

    # bitianist_manual_tagger = generate_tagger_dictionary(
    #     'bitianist-manual-tagger',
    #     show_name='Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù† Ø¯Ø³ØªÛŒ Ø¨ÛŒØªÛŒØ§Ù†ÛŒØ³Øª',
    #     owner='bitianist',
    #     tag_set='bitianist-tag-set',
    #     is_automatic=False,
    #     model_details=model_details
    # )

    # response, error = post(taggers_url, bitianist_manual_tagger)
    # if error:
    #     return 0

    # # 3
    # model_details = {
    #     'module': 'nltk',
    #     'type': 'hybrid',
    #     'state': 'not-ready',
    # }

    # bitianist_refinement_tagger = generate_tagger_dictionary(
    #     'bitianist-refinement-tagger',
    #     show_name='Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù† Ù¾Ø§Ù„Ø§ÛŒØ´',
    #     owner='bitianist',
    #     tag_set='bitianist-tag-set',
    #     is_automatic=True,
    #     model_details=model_details
    # )

    # response, error = post(taggers_url, bitianist_refinement_tagger)
    # if error:
    #     return 0

    
    # 4
    model_details = {
        'module': 'nltk',
        'type': 'hybrid',
        'state': 'not-ready',
    }

    bitianist_seq2seq_tagger = generate_tagger_dictionary(
        'bitianist-seq2seq-tagger',
        show_name='Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù† ØªÙˆØ§Ù„ÛŒâ€ŒØ¨Ù‡â€ŒØªÙˆØ§Ù„ÛŒ',
        owner='bitianist',
        tag_set='bitianist-tag-set',
        is_automatic=True,
        model_details=model_details
    )

    response, error = post(taggers_url, bitianist_seq2seq_tagger)
    if error:
        return 0



@utils.time_usage(logger)
def import_text_equivalents():
    df = pd.read_excel(text_equivalents_path, sheet_name='main')
    text_content, text_normal_content = '', ''
    text = None
    logger.info(f'>> Reading text_equivalents : {df.columns}')
    normalizer = 'bitianist-manual-normalizer'
    # manual_normalizer = generate_normalizer_dictionary('manual-normalizer')
    i = 0
    for i in df.index:

        text_content = df['Ù…ØªÙ† ØºÛŒØ± Ø±Ø³Ù…ÛŒ'][i]
        if text_content.__str__() == 'nan' or text_content.__str__().isspace():
            break

        text_normal_content = df['Ù…ØªÙ† Ø±Ø³Ù…ÛŒ'][i]
        if text_normal_content.__str__() == 'nan' or text_normal_content.__str__().isspace():
            break

        text = generate_text_dictionary(text_content)
        text_normal = generate_text_normal_dictionary(text_normal_content, normalizer, text)
        response, error = post(text_normals_url, text_normal)
        if error:
            break
        if i % 25 == 0:
            logger.info(f'> Item {i} imported.')
    logger.info(f'> Items count : {i+1}')

@utils.time_usage(logger)
def import_word_equivalents():
    df = pd.read_excel(word_equivalents_path, sheet_name='main')
    word_content, word_normal_content = '', ''
    word = None
    logger.info(f'>> Reading word_equivalents : {df.columns}')
    ctr = 1
    normalizer = 'bitianist-manual-normalizer'
    word_content_set = set()
    for i in df.index:

        word_content = df['Ú©Ù„Ù…Ù‡ ØºÛŒØ± Ø±Ø³Ù…ÛŒ'][i].__str__().strip()
        if word_content in word_content_set:
            continue
        else:
            word_content_set.add(word_content)

        if word_content == 'nan' or word_content.isspace():
            break

        word_normal_content = df['Ú©Ù„Ù…Ù‡ Ø±Ø³Ù…ÛŒ'][i].__str__().strip()
        if word_normal_content == 'nan' or word_normal_content.isspace():
            break

        word = generate_word_dictionary(word_content)
        word_normal = generate_word_normal_dictionary(word_normal_content, normalizer, word)
        response, error = post(word_normals_url, word_normal)
        if error:
            break
        ctr += 1
        if ctr % 25 == 0:
            logger.info(f'> Item {ctr} imported.')
    logger.info(f'> Items count : {ctr}')


# @utils.time_usage(logger)
def read_bijankhan_xml_file(xml_file):
    text_tag = None
    logger = utils.get_logger(logger_name='data_importer')
    token_content, tag_name, text_content = '', '', ''
    tag, tagged_token = None, None
    text_tag_tagged_tokens = []
    # texts_count = 0
    try:
        with open(xml_file, mode="r", encoding="utf-8") as xf:
            file_name = os.path.basename(xml_file)
            # logger.info(f'> Importing xml file "{file_name}"')
            xml_string = xf.read()
            tree = ET.ElementTree(ET.fromstring(xml_string))
            root = tree.getroot()
            for tagged_token_xml in root.findall('*'):
                token_content = tagged_token_xml.find('w').text.strip().replace(' ', 'â€Œ')
                text_content += token_content + ' '
                tag_name = tagged_token_xml.find('tag').text[0]
                tag = generate_tag_dictionary(name=tag_name)
                tagged_token = generate_tagged_token_dictionary(token_content, tag)
                text_tag_tagged_tokens.append(tagged_token)

            text = generate_text_dictionary(text_content)
            text_tag = generate_text_tag_dictionary(
                        tagged_tokens=text_tag_tagged_tokens,
                        tagger='bijankhan-manual-tagger', 
                        text=text)

            logger.info(f'> File {file_name} reading finished.')
            return text_tag
    except IOError as exc:
        if exc.errno != errno.EISDIR:
            logger.exception(exc)

# @utils.time_usage(logger)
def send_bijankhan_text_tag(text_tag):
    response, error = post(text_tags_url, text_tag)
    if error:
        return

@utils.time_usage(logger)
def import_bijankhan_data():
    logger.info(f'>> Reading bijankhan data')
    files = glob.glob(fr'{bijankhan_data_dir}/*.xml')
    # text_tags = []
    # files = files[0:2]

    text_tags = Parallel(n_jobs=-1, verbose=20)(delayed(read_bijankhan_xml_file)(xml_file) for xml_file in files)
    logger.info(f'>> Total {len(text_tags)} texts read.')
    
    Parallel(n_jobs=24, verbose=20, backend='threading')(delayed(send_bijankhan_text_tag)(text_tag) for text_tag in text_tags)
    logger.info(f'>> Total {len(text_tags)} texts imported.')

def import_bitianist_text_tag(text_tag_id=None):
    tagger = 'bitianist-manual-tagger'

    # 1
    text_content = 'Ø´Ù„ÙˆØºÛŒ ÙØ±Ù‡Ù†Ú¯â€ŒØ³Ø±Ø§ Ø¢ÛŒØ¯ÛŒ Ø§Ù†Ù‚Ø¯Ø± Ø§ÙˆÙˆØ±Ø¯ Ø§ÙˆÙˆØ±Ø¯Ù† Ù…Ù†Ùˆ Ù…ÛŒØ¯ÙˆÙ† Ø®ÙˆÙ†Ù‡ Ø¬ÙˆÙˆÙ† Ø²Ù…ÙˆÙ†Ù‡ Ù†ÙˆÙ† Ù…Ø³Ù„Ù…ÙˆÙ† Ú©ØªØ§Ø¨Ø®ÙˆÙ†Ù‡ Ø¯Ù†Ø¯ÙˆÙ†'
    text_content += ' Ù†Ø´ÙˆÙ† Ù¾Ø§Ø³ØªØ§ Ù¾Ù†Ù‡ ØªØ§Ú† ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒØ¯ Ø³ÛŒâ€ŒÙ¾ÛŒâ€ŒÛŒÙˆâ€Œ Ø³ÛŒâ€ŒÙ¾ÛŒâ€ŒÛŒÙˆâ€Œâ€ŒÙ‡Ø§ Ú¯Ø±Ø§ÙÛŒÚ© Ø§ÙˆÙ…Ø¯Ù† Ù…ÛŒâ€ŒØ®Ø§Ù† ÙˆØ§Ø³ Ùª'

    tagged_tokens = [
        generate_tagged_token_dictionary('Ø´Ù„ÙˆØºÛŒ', generate_tag_dictionary(name='A')),
        generate_tagged_token_dictionary('ÙØ±Ù‡Ù†Ú¯â€ŒØ³Ø±Ø§', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø¢ÛŒØ¯ÛŒ', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø§Ù†Ù‚Ø¯Ø±', generate_tag_dictionary(name='D')),
        generate_tagged_token_dictionary('Ø§ÙˆÙˆØ±Ø¯', generate_tag_dictionary(name='V')),
        generate_tagged_token_dictionary('Ø§ÙˆÙˆØ±Ø¯Ù†', generate_tag_dictionary(name='V')),
        generate_tagged_token_dictionary('Ù…Ù†Ùˆ', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù…ÛŒØ¯ÙˆÙ†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø®ÙˆÙ†Ù‡', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø¬ÙˆÙˆÙ†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø²Ù…ÙˆÙ†Ù‡', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù†ÙˆÙ†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù…Ø³Ù„Ù…ÙˆÙ†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ú©ØªØ§Ø¨Ø®ÙˆÙ†Ù‡', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø¯Ù†Ø¯ÙˆÙ†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù†Ø´ÙˆÙ†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù¾Ø§Ø³ØªØ§', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù¾Ù†Ù‡', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('ØªØ§Ú†', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('ØªÙ†Ø¸ÛŒÙ…Ø§Øª', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒØ¯', generate_tag_dictionary(name='V')),
        generate_tagged_token_dictionary('Ø³ÛŒâ€ŒÙ¾ÛŒâ€ŒÛŒÙˆâ€Œ', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø³ÛŒâ€ŒÙ¾ÛŒâ€ŒÛŒÙˆâ€Œâ€ŒÙ‡Ø§', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ú¯Ø±Ø§ÙÛŒÚ©', generate_tag_dictionary(name='N')),
        generate_tagged_token_dictionary('Ø§ÙˆÙ…Ø¯Ù†', generate_tag_dictionary(name='V')),
        generate_tagged_token_dictionary('Ù…ÛŒâ€ŒØ®Ø§Ù†', generate_tag_dictionary(name='V')),
        generate_tagged_token_dictionary('ÙˆØ§Ø³', generate_tag_dictionary(name='E')),
        generate_tagged_token_dictionary('Ùª', generate_tag_dictionary(name='O')),

    ]

    text_content += ' kb m kg g cm mm'
    tagged_tokens += [
        generate_tagged_token_dictionary('kb', generate_tag_dictionary(name='L')),
        generate_tagged_token_dictionary('m', generate_tag_dictionary(name='L')),
        generate_tagged_token_dictionary('kg', generate_tag_dictionary(name='L')),
        generate_tagged_token_dictionary('g', generate_tag_dictionary(name='L')),
        generate_tagged_token_dictionary('cm', generate_tag_dictionary(name='L')),
        generate_tagged_token_dictionary('mm', generate_tag_dictionary(name='L')),

    ]
    logger.info(f'> text_content : \n{text_content}\n')
    logger.info(f'> tagged_tokens : \n{tagged_tokens}\n')
    text = generate_text_dictionary(text_content)
    text_tag = generate_text_tag_dictionary(
                tagged_tokens=tagged_tokens,
                tagger=tagger,
                text=text)

    response, error = None, None
    if text_tag_id:
        response, error = put(f'{text_tags_url}/{text_tag_id}', text_tag)
    else:
        response, error = post(text_tags_url, text_tag)
        logger.info(response)
    if error:
        return

def import_bitianist_evaluate_text_tag(text_tag_id=None):
    tagger = 'bitianist-manual-tagger'

    text_content = '''
Ø¨Ø±ÛŒÙ€Ù€Ù€Ù€Ù€Ù€Ù€Ø¯ Ø¨Ù‡ Ø±Ø³ØªÙˆØ±Ø§Ù†https://chilivery.com/tehran/restaurant1/ .... ØºØ°Ø§Ø´ Ø®ÛŒÛŒÛŒÙ„ÛŒ Ø¹Ø§Ø§Ø§Ø§Ø§Ù„ÛŒÙ‡ğŸ˜ğŸ˜ğŸ˜
Ø§Ø² Ù†Ø¸Ø± Ù‚ÛŒÙ…Øª Ù…ÛŒØ§Ø±Ø²Ù‡ Ùƒ Ø¨Ø±Ø¦ ÙˆÙ„ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø®ÙØ³Øª
Ù…Ø« Ø§ÛŒÙ† Ø¬Ø§ÛŒ Ø¯ÛŒÚ¯Ù‡ Ø§ÛŒ Ù†Ø¯ÛŒØ¯Ù… Ù…Ø´ØªØ±ÛŒØ§Ø´ Ø§Ù†Ù‚Ø¯Ø± Ø²ÛŒØ§Ø¯ Ø¨Ø§Ø´Ù‡.
Ú©ÛŒÙÛŒØª Ù¾ÛŒØªØ²Ø§Ù‡Ø§Ø´ Ø§ØµÙ„Ù† Ø®ÙˆØ¨ Ù†ÛŒØ³Øª... Ø§Ø² Ø´Ù„ÙˆØºÛŒÙ‡ Ù…ÛŒØ¯ÙˆÙ† ØªØ¬Ø±ÛŒØ´ Ø¯Ø§Ø±Ù‡ Ù¾ÙˆÙ„ Ø¯Ø±Ù…ÛŒØ§Ø±Ù‡.
Ø¨Ø§Ø²Ù… Ø§Ø²Ø´ Ø®Ø±ÛŒØ¯ Ù…ÛŒÚ©Ù†Ù…ØŒ Ø¨Ù‡ØªØ±ÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÛŒÙ† Ù…Ù†Ø·Ù‚Ø³Øª.
Ø§Ú¯Ù‡ ÛŒÙ‡ Ø¨Ø§Ø± Ø¨Ø±ÛŒ Ø§ÙˆÙ†Ø¬Ø§ Ù…Ø´ØªØ±ÛŒØ´ Ù…ÛŒØ´ÛŒ.
Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø±Ø²ÙˆÙ…Ù‡ Ø¨Ù‡ Ø¢Ø¯Ø±Ø³ job@gmail.com Ù…ÛŒÙ„ Ø¨Ø²Ù†ÛŒØ¯.
Ú©ØªØ§Ø¨Ù‡ Ø±Ùˆ ØªÙˆ ÙØ±Ù‡Ù†Ú¯ Ø³Ø±Ø§ Ù¾ÛŒØ¯Ø§Ø´ Ú©Ø±Ø¯Ù….
Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ú©Ù„ Ø¨Ù‡ Ø¢ÛŒØ¯ÛŒ @bitianist Ù¾ÛŒØ§Ù… Ø¨Ø¯Ù‡ÛŒØ¯.
Ø­Ø§Ø¶Ø±Ù… Ø´Ø±Ø· Ø¨Ø¨Ù†Ø¯Ù… Ù‡Ù…ÙˆÙ†Ùˆ ÙˆØ±Ø¯Ø§Ø´ØªÙ† ØªØ²Ø¦ÛŒÙ† Ú©Ø±Ø¯Ù† Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ø§ÙˆÙˆØ±Ø¯Ù†!!!!
Ø¢Ø®Ù‡ ÙˆØ§Ù‚Ø¹Ø§ Ø¯Ø±Ø³ØªÙ‡ Ø§ÛŒÙ†Ú©Ø§Ø±ØŸØŸØŸ Ú†Ø±Ø§ Ø®Ø¨ Ø¯Ø±Ø³Øª Ù†Ù…ÛŒÚ©Ù†Ù†ØŸØŸ
    '''
    # 1
    normal_text_content = '''
Ø¨Ø±ÛŒØ¯ Ø¨Ù‡ Ø±Ø³ØªÙˆØ±Ø§Ù† https://chilivery.com/tehran/restaurantÛ±/ . ØºØ°Ø§ Ø´ Ø®ÛŒÙ„ÛŒ Ø¹Ø§Ù„ÛŒÙ‡ ğŸ˜ğŸ˜ğŸ˜
Ø§Ø²â€ŒÙ†Ø¸Ø± Ù‚ÛŒÙ…Øª Ù…ÛŒØ§Ø±Ø²Ù‡ Ú© Ø¨Ø±ÛŒ ÙˆÙ„ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø®ÙØ³Øª
Ù…Ø« Ø§ÛŒÙ† Ø¬Ø§ÛŒ Ø¯ÛŒÚ¯Ù‡â€ŒØ§ÛŒ Ù†Ø¯ÛŒØ¯Ù… Ù…Ø´ØªØ±ÛŒ Ø§Ø´ Ø§Ù†Ù‚Ø¯Ø± Ø²ÛŒØ§Ø¯ Ø¨Ø§Ø´Ù‡ .
Ú©ÛŒÙÛŒØª Ù¾ÛŒØªØ²Ø§Ù‡Ø§Ø´ Ø§ØµÙ„Ù† Ø®ÙˆØ¨ Ù†ÛŒØ³Øª â€¦ Ø§Ø² Ø´Ù„ÙˆØº ÛŒÙ‡ Ù…ÛŒØ¯ÙˆÙ† ØªØ¬Ø±ÛŒØ´ Ø¯Ø§Ø±Ù‡ Ù¾ÙˆÙ„ Ø¯Ø±Ù…ÛŒØ§Ø±Ù‡ .
Ø¨Ø§Ø² Ù… Ø§Ø² Ø´ Ø®Ø±ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù… ØŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ù†Ø·Ù‚ Ø³Øª .
Ø§Ú¯Ù‡ ÛŒÙ‡ Ø¨Ø§Ø± Ø¨Ø±ÛŒ Ø§ÙˆÙ†Ø¬Ø§ Ù…Ø´ØªØ±ÛŒ Ø´ Ù…ÛŒØ´ÛŒ .
Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø±Ø²ÙˆÙ…Ù‡ Ø¨Ù‡ Ø¢Ø¯Ø±Ø³ job@gmail.com Ù…ÛŒÙ„ Ø¨Ø²Ù†ÛŒØ¯ .
Ú©ØªØ§Ø¨Ù‡ Ø±Ùˆ ØªÙˆ ÙØ±Ù‡Ù†Ú¯â€ŒØ³Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ø´ Ú©Ø±Ø¯Ù… .
Ø¯Ø±â€ŒØµÙˆØ±Øª Ù…Ø´Ú©Ù„ Ø¨Ù‡ Ø¢ÛŒØ¯ÛŒ @bitianist Ù¾ÛŒØ§Ù… Ø¨Ø¯Ù‡ÛŒØ¯ .
Ø­Ø§Ø¶Ø± Ù… Ø´Ø±Ø· Ø¨Ø¨Ù†Ø¯Ù… Ù‡Ù…ÙˆÙ† Ùˆ ÙˆØ±Ø¯Ø§Ø´ØªÙ† ØªØ²ÛŒÛŒÙ† Ú©Ø±Ø¯Ù† Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ø§ÙˆÙˆØ±Ø¯Ù† !
Ø¢Ø®Ù‡ ÙˆØ§Ù‚Ø¹Ø§ Ø¯Ø±Ø³ØªÙ‡ Ø§ÛŒÙ† Ú©Ø§Ø± ØŸ Ú†Ø±Ø§ Ø®Ø¨ Ø¯Ø±Ø³Øª Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù† ØŸ
'''.strip()

    tags_string = '''
V E N K O N C D A X O 
E N V J V J D V O 
D Z N Z V N C D A V O O 
N N D A V O E A C N N V N V O O 
D C E C N V O A N C O O 
J U N V D N C V O O 
E N N E N M N V O O 
N P N A A C V O O 
E N E N S N V O O 
A V N V Z J V N V J E Z V O O 
D D A T N O D D A V O
    '''.strip()

    '''
Ø¨Ø±ÛŒØ¯_V  Ø¨Ù‡_E  Ø±Ø³ØªÙˆØ±Ø§Ù†_N  https://chilivery.com/tehran/restaurantÛ±/_K  ._O  ØºØ°Ø§_N  Ø´_C  Ø®ÛŒÙ„ÛŒ_D  Ø¹Ø§Ù„ÛŒÙ‡_A  ğŸ˜ğŸ˜ğŸ˜_X  
Ø§Ø²â€ŒÙ†Ø¸Ø±_E  Ù‚ÛŒÙ…Øª_N  Ù…ÛŒØ§Ø±Ø²Ù‡_N_V  Ú©_C_J  Ø¨Ø±ÛŒ_V  ÙˆÙ„ÛŒ_J  ÙˆØ§Ù‚Ø¹Ø§_D  Ø®ÙØ³Øª_N_V  
Ù…Ø«_N_D  Ø§ÛŒÙ†_T_Z  Ø¬Ø§ÛŒ_N  Ø¯ÛŒÚ¯Ù‡â€ŒØ§ÛŒ_N_Z  Ù†Ø¯ÛŒØ¯Ù…_V  Ù…Ø´ØªØ±ÛŒ_N  Ø§Ø´_C  Ø§Ù†Ù‚Ø¯Ø±_D  Ø²ÛŒØ§Ø¯_A  Ø¨Ø§Ø´Ù‡_V  ._O  
Ú©ÛŒÙÛŒØª_N  Ù¾ÛŒØªØ²Ø§Ù‡Ø§Ø´_N  Ø§ØµÙ„Ù†_N_D  Ø®ÙˆØ¨_A  Ù†ÛŒØ³Øª_V  â€¦_O  Ø§Ø²_E  Ø´Ù„ÙˆØº_A  ÛŒÙ‡_C  Ù…ÛŒØ¯ÙˆÙ†_N  ØªØ¬Ø±ÛŒØ´_N  Ø¯Ø§Ø±Ù‡_V  Ù¾ÙˆÙ„_N  Ø¯Ø±Ù…ÛŒØ§Ø±Ù‡_N_V  ._O  
Ø¨Ø§Ø²_D  Ù…_C  Ø§Ø²_E  Ø´_C  Ø®Ø±ÛŒØ¯_N  Ù…ÛŒâ€ŒÚ©Ù†Ù…_V  ØŒ_O  Ø¨Ù‡ØªØ±ÛŒÙ†_A  Ù…Ù†Ø·Ù‚_N  Ø³Øª_C  ._O  
Ø§Ú¯Ù‡_J  ÛŒÙ‡_U  Ø¨Ø§Ø±_N  Ø¨Ø±ÛŒ_V  Ø§ÙˆÙ†Ø¬Ø§_D  Ù…Ø´ØªØ±ÛŒ_N  Ø´_C  Ù…ÛŒØ´ÛŒ_A_V  ._O  
Ø¨Ø±Ø§ÛŒ_E  Ø§Ø±Ø³Ø§Ù„_N  Ø±Ø²ÙˆÙ…Ù‡_N  Ø¨Ù‡_E  Ø¢Ø¯Ø±Ø³_N  job@gmail.com_M  Ù…ÛŒÙ„_N  Ø¨Ø²Ù†ÛŒØ¯_V  ._O  
Ú©ØªØ§Ø¨Ù‡_R_N  Ø±Ùˆ_N_P  ØªÙˆ_Z_N  ÙØ±Ù‡Ù†Ú¯â€ŒØ³Ø±Ø§_A  Ù¾ÛŒØ¯Ø§_A  Ø´_C  Ú©Ø±Ø¯Ù…_V  ._O  
Ø¯Ø±â€ŒØµÙˆØ±Øª_E  Ù…Ø´Ú©Ù„_N  Ø¨Ù‡_E  Ø¢ÛŒØ¯ÛŒ_N  @bitianist_S  Ù¾ÛŒØ§Ù…_N  Ø¨Ø¯Ù‡ÛŒØ¯_V  ._O  
Ø­Ø§Ø¶Ø±_A  Ù…_C_V  Ø´Ø±Ø·_N  Ø¨Ø¨Ù†Ø¯Ù…_V  Ù‡Ù…ÙˆÙ†_Z  Ùˆ_J  ÙˆØ±Ø¯Ø§Ø´ØªÙ†_V  ØªØ²ÛŒÛŒÙ†_N  Ú©Ø±Ø¯Ù†_V  Ùˆ_J  Ø¨Ø±Ø§ÛŒ_E  Ù…Ø§_Z  Ø§ÙˆÙˆØ±Ø¯Ù†_V  !_O  
Ø¢Ø®Ù‡_D  ÙˆØ§Ù‚Ø¹Ø§_D  Ø¯Ø±Ø³ØªÙ‡_D_A  Ø§ÛŒÙ†_T  Ú©Ø§Ø±_N  ØŸ_O  Ú†Ø±Ø§_D  Ø®Ø¨_D  Ø¯Ø±Ø³Øª_A  Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†_V  ØŸ_O
    '''

    normal_text_content = normal_text_content.replace('\n', ' \\n ')
    normal_text_content = re.sub(' +', ' ', normal_text_content).strip()
    token_contents_tuple = normal_text_content.split(' ')

    tags_string = tags_string.replace('\n', ' ')
    tags_string = re.sub(' +', ' ', tags_string).strip()
    tags_string_tuple = tags_string.split(' ')


    logger.info(f'token_contents_tuple {len(token_contents_tuple)} : \n{token_contents_tuple}\n')
    logger.info(f'tags_string_tuple {len(tags_string_tuple)} : \n{tags_string_tuple}\n')

    tagged_tokens = zip(token_contents_tuple, tags_string_tuple)
    # new_line = '\n'
    # logger.info(f'> tagged_tokens : \n {new_line.join([tagged_token.__str__() for tagged_token in tagged_tokens])}\n')
    tagged_tokens = [generate_tagged_token_dictionary(tagged_token[0], generate_tag_dictionary(name=tagged_token[1])) for tagged_token in tagged_tokens]
    text = generate_text_dictionary(text_content)
    text_tag = generate_text_tag_dictionary(
                tagged_tokens=tagged_tokens,
                tagger=tagger,
                text=text)

    logger.info(f'text_tag : \n{text_tag}')
    response, error = None, None
    if text_tag_id:
        response, error = put(f'{text_tags_url}/{text_tag_id}', text_tag)
    else:
        response, error = post(text_tags_url, text_tag)
        logger.info(response)
    if error:
        return

def main():
    try:
        # import_tag_sets()
        # import_validators()
        import_normalizers()
        import_taggers()

        
        # import_bijankhan_data()
        # import_text_equivalents()
        # import_word_equivalents()

        # import_bitianist_text_tag('8f40d06e-aa29-46ed-8aed-82c942598be6')
        # import_bitianist_evaluate_text_tag()

        # import_tags()
        # import_translation_characters()
        # import_refinement_patterns()
    except Exception as e:
        logger.exception(e)

if __name__ == "__main__": main()




# # @utils.time_usage(logger)
# def read_bijankhan_xml_file(xml_file):
#     tagged_sentences = []
#     logger = utils.get_logger(logger_name='data_importer')
#     xml_string, token_content, tag_name, sentence_content = '', '', '', ''
#     tag, token, sentence, tagged_sentence = None, None, None, None
#     sentence_tokens = []
#     sentence_breaker_pattern = re.compile(r'[!\.\?â¸®ØŸ]')
#     # sentences_count = 0
#     try:
#         with open(xml_file, mode="r", encoding="utf-8") as xf:
#             file_name = os.path.basename(xml_file)
#             # logger.info(f'> Importing xml file "{file_name}"')
#             xml_string = xf.read()
#             tree = ET.ElementTree(ET.fromstring(xml_string))
#             root = tree.getroot()
#             for tagged_token_xml in root.findall('*'):
#                 token_content = tagged_token_xml.find('w').text.strip().replace(' ', 'â€Œ')
#                 sentence_content += token_content + ' '
#                 tag_name = tagged_token_xml.find('tag').text[0]
#                 tag = generate_tag_dictionary(name=tag_name)
#                 token = generate_tagged_token_dictionary(token_content, tag)
#                 sentence_tokens.append(token)
#                 if len(token_content) == 1 and sentence_breaker_pattern.search(token_content):
#                     sentence = generate_sentence_dictionary(sentence_content)
#                     tagged_sentence = generate_tagged_sentence_dictionary(
#                                 tokens=sentence_tokens,
#                                 tagger='bijankhan-manual-tagger', 
#                                 sentence=sentence)
#                     tagged_sentences.append(tagged_sentence)
#                     # sentences_count += 1
#                     sentence_content = ''
#                     sentence_tokens = []
#                     # if sentences_count % 25 == 0:
#                         # logger.info(f'> File {file_name} now has {sentences_count} imported sentence ...')

#             logger.info(f'> File {file_name} has {len(tagged_sentences)} sentences.')
#             return tagged_sentences
#     except IOError as exc:
#         if exc.errno != errno.EISDIR:
#             logger.exception(exc)

# # @utils.time_usage(logger)
# def send_bijankhan_tagged_sentence(tagged_sentence):
#     response, error = post(tagged_sentences_url, tagged_sentence)
#     if error:
#         return

# @utils.time_usage(logger)
# def import_bijankhan_data():
#     # global tagged_sentences, tagged_tokens, token_set, word_set, tag_example_dic
#     logger.info(f'>> Reading bijankhan data')
#     files = glob.glob(fr'{bijankhan_data_dir}/*.xml')
#     tagged_sentences, tagged_sentences_list = [], []
#     # files = files[0:1]
#     # num_cores = multiprocessing.cpu_count()
#     # logger.info(f'> Number of cup cores : {num_cores}')
#     # tagged_sentences.extend(
#     tagged_sentences_list = Parallel(n_jobs=-1, verbose=20)(delayed(read_bijankhan_xml_file)(xml_file) for xml_file in files)
#     # )
#     for current_tagged_sentences in tagged_sentences_list:
#         tagged_sentences.extend(current_tagged_sentences)
#     logger.info(f'>> Total {len(tagged_sentences)} sentences added.')
    
#     Parallel(n_jobs=24, verbose=20, backend='threading')(delayed(send_bijankhan_tagged_sentence)(tagged_sentence) for tagged_sentence in tagged_sentences)
    
#     logger.info(f'>> {len(tagged_sentences)} sentences imported.')



# def import_tags():
#     """ 
#     https://htmlcolorcodes.com/
#     """
#     tags = (
#         ('E', 'E', '#E74C3C'),
#         ('N', 'Ø§Ø³Ù…', '#3498DB'),
#         ('V', 'ÙØ¹Ù„', '#9B59B6'),

#         ('J', 'J', '#1ABC9C'),
#         ('A', 'ØµÙØª', '#F1C40F'),
#         ('U', 'Ø¹Ø¯Ø¯', '#E67E22'),
        
#         ('T', 'T', '#ECF0F1'),
#         ('Z', 'Ø¶Ù…ÛŒØ±', '#BDC3C7'),
#         ('O', 'Ø¹Ù„Ø§Ù…Øª', '#7F8C8D'),
        
#         ('L', 'L', '#34495E'),
#         ('P', 'Ø­Ø±Ù Ø§Ø¶Ø§ÙÙ‡ Ù¾Ø³ÛŒÙ†', '#C39BD3'),
#         ('D', 'Ù‚ÛŒØ¯', '#FBFCFC'),
        
#         ('C', 'C', '#0E6655'),
#         ('R', 'R', '#922B21'),
#         ('I', 'Ø­Ø±Ù Ù†Ø¯Ø§', '#AED6F1'),

#     )
#     for tag_data in tags:
#         tag = generate_tag_dictionary(tag_data[0])
#         response, error = post(tag_url, tag)
#         if error:
#             continue

# def import_translation_characters():
#     translation_characters = (
#         # (r'Â ', r' ', 'space character 160 -> 32', 'hazm', 'true'),
#         # (r'Ùƒ', r'Ú©', '', 'hazm', 'true'),
#         # (r'ÙŠ', r'ÛŒ', '', 'hazm', 'true'),
#         # (r'â€œ', r'\"', '', 'hazm', 'true'),
#         # (r'â€', r'\"', '', 'hazm', 'true'),
#         # (r'0', r'Û°', '', 'hazm', 'true'),
#         # (r'1', r'Û±', '', 'hazm', 'true'),
#         # (r'2', r'Û²', '', 'hazm', 'true'),
#         # (r'3', r'Û³', '', 'hazm', 'true'),
#         # (r'4', r'Û´', '', 'hazm', 'true'),
#         # (r'5', r'Ûµ', '', 'hazm', 'true'),
#         # (r'6', r'Û¶', '', 'hazm', 'true'),
#         # (r'7', r'Û·', '', 'hazm', 'true'),
#         # (r'8', r'Û¸', '', 'hazm', 'true'),
#         # (r'9', r'Û¹', '', 'hazm', 'true'),
#         # (r'%', r'Ùª', '', 'hazm', 'true'),
#         (r'?', r'ØŸ', '', 'hazm', 'true'),
#     )
#     url = 'http://127.0.0.1:8000/mohaverekhan/api/rules/translation_character/'
#     for translation_character in translation_characters:
#         data = f'''
# {{
#     "source": "{translation_character[0]}",
#     "destination": "{translation_character[1]}",
#     "description": "{translation_character[2]}",
#     "owner": "{translation_character[3]}",
#     "is_valid": {translation_character[4]}
# }}'''
#         response, error = post(url, data)
#         if error:
#             continue
#         # response = requests.post(url, data=data.encode('utf-8'), headers={'Content-type': 'application/json; charset=utf-8'})
#         # if response.status_code != 200 and response.status_code != 201:
#         #     logger.info(f'> Error : {response.status_code} {response.text}')
#         #     response.raise_for_status()
#         #     break
#         # logger.info(f'{response.status_code} : {response.text}')

# def import_refinement_patterns():
#     punctuations = r'\.:!ØŒØ›ØŸÂ»\]\)\}Â«\[\(\{\'\"â€¦'
#     refinement_patterns = (
#         (r'\.{4,}', r'.', 'replace more than 3 dots with 1 dot', 0, 'bitianist', 'true'),
#         (r'( \.{2})|(\.{2} )', r' . ', 'replace exactly 2 dots with 1 dot', 0, 'bitianist', 'true'),
#         (r' ?\.\.\.', r' â€¦', 'replace 3 dots', 0, 'hazm', 'true'),
#         (r'(.)\1{1,}', r'\1', 'remove repetitions', 0, 'bitianist', 'true'),
#         (r'[Ù€\r]', r'', 'remove keshide', 0, 'hazm', 'true'),
#         (r'[\u064B\u064C\u064D\u064E\u064F\u0650\u0651\u0652]', r'', 'remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN', 0, 'hazm', 'true'),
#         (r'"([^\n"]+)"', r'Â«\1Â»', 'replace quotation with gyoome', 0, 'hazm', 'true'),
#         (rf'([{punctuations}])', r' \1 ', 'add extra space before and after of punctuations', 0, 'bitianist', 'true'),
#         (r' +', r' ', 'remove extra spaces', 0, 'hazm', 'true'),
#         (r'\n+', r'\n', 'remove extra newlines', 0, 'bitianist', 'true'),
#         (r'([^ ]Ù‡) ÛŒ ', r'\1â€ŒÛŒ ', 'before ÛŒ - replace space with non-joiner ', 0, 'hazm', 'true'),
#         (r'(^| )(Ù†?Ù…ÛŒ) ', r'\1\2â€Œ', 'after Ù…ÛŒØŒÙ†Ù…ÛŒ - replace space with non-joiner ', 0, 'hazm', 'true'),
#         (rf'(?<=[^\n\d {punctuations}]{2}) (ØªØ±(ÛŒÙ†?)?|Ú¯Ø±ÛŒ?|Ù‡Ø§ÛŒ?)(?=[ \n{punctuations}]|$)', r'â€Œ\1', 'before ØªØ±, ØªØ±ÛŒ, ØªØ±ÛŒÙ†, Ú¯Ø±, Ú¯Ø±ÛŒ, Ù‡Ø§, Ù‡Ø§ÛŒ - replace space with non-joiner', 0, 'hazm', 'true'),
#         (rf'([^ ]Ù‡) (Ø§(Ù…|ÛŒÙ…|Ø´|Ù†Ø¯|ÛŒ|ÛŒØ¯|Øª))(?=[ \n{punctuations}]|$)', r'\1â€Œ\2', 'before Ø§Ù…, Ø§ÛŒÙ…, Ø§Ø´, Ø§Ù†Ø¯, Ø§ÛŒ, Ø§ÛŒØ¯, Ø§Øª - replace space with non-joiner', 0, 'hazm', 'true'),  
#         # (r'', r'', '', 0, 'hazm', 'true'),
#         # (r'', r'', '', 0, 'hazm', 'true'),
#         # (r'', r'', '', 0, 'hazm', 'true'),
#         # (r'', r'', '', 0, 'hazm', 'true'),
#         # (r'', r'', '', 0, 'hazm', 'true'),
#         # (r'', r'', '', 0, 'hazm', 'true'),
#     )
#     for refinement_pattern_data in refinement_patterns:
#         refinement_pattern = generate_refinement_pattern_dictionary(
#             pattern=refinement_pattern_data[0],
#             replacement=refinement_pattern_data[1],
#             description=refinement_pattern_data[2],
#             order=refinement_pattern_data[3],
#             owner=refinement_pattern_data[4],
#             is_valid=refinement_pattern_data[5]
#         )
     
#         response, error = post(refinement_pattern_url, refinement_pattern)
#         if error:
#             return




# def import_bijankhan_xml_file(xml_file):
#     logger = utils.get_logger(logger_name='data-importer')
#     xml_string, token_content, tag_name, sentence_content, text_content = '', '', '', '', ''
#     tag, token, sentence, text = None, None, None, None
#     sentence_tokens, text_sentences = [], []
#     sentence_breaker_pattern = re.compile(r'[!\.\?â¸®ØŸ]')
#     try:
#         with open(xml_file, mode="r", encoding="utf-8") as xf:
#             file_name = os.path.basename(xml_file)
#             logger.info(f'> Importing xml file "{file_name}"')
#             xml_string = xf.read()
#             tree = ET.ElementTree(ET.fromstring(xml_string))
#             root = tree.getroot()
#             text_sentences = []
#             text_content = ''
#             for tagged_token_xml in root.findall('*'):
#                 token_content = tagged_token_xml.find('w').text
#                 sentence_content += token_content + ' '
#                 tag_name = tagged_token_xml.find('tag').text[0]
#                 tag = generate_tag_dictionary(tag_name)
#                 token = generate_tagged_token_dictionary(token_content, tag)
#                 sentence_tokens.append(token)
#                 if len(token_content) == 1 and sentence_breaker_pattern.search(token_content):
#                     sentence = generate_sentence_dictionary(sentence_content, sentence_tokens)
#                     text_sentences.append(sentence)
#                     text_content += sentence_content
#                     sentence_content = ''
#                     sentence_tokens = []
                
#             manual_tagger = generate_manual_tagger_dictionary(text_sentences)
#             text = generate_text_dictionary(text_content, manual_tagger)
#             response, error = post(text_url, text)
#             if error:
#                 return 0
#             logger.info(f'> File {file_name} finished. {len(text_sentences)} sentences added.')
#             return len(text_sentences)
#     except IOError as exc:
#         if exc.errno != errno.EISDIR:
#             logger.exception(exc)

# def generate_sentence_dictionary(content, text=None, id=None):
#     d = {}
#     d['content'] = content
#     if text:
#         d['text'] = text 
#     if id:
#         d['id'] = id 
#     return d






# def generate_normal_sentence_dictionary(content, normalizer, 
#         sentence, id=None):
#     d = {}
#     d['content'] = content
#     d['normalizer'] = normalizer
#     d['sentence'] = sentence
#     if id:
#         d['id'] = id 
#     return d



# def generate_tagged_sentence_dictionary(tokens, tagger, 
#         sentence, id=None):
#     d = {}
#     d['tokens'] = tokens
#     d['tagger'] = tagger
#     d['sentence'] = sentence
#     if id:
#         d['id'] = id 
#     return d

# def generate_translation_character_dictionary(source, destination, owner=None, 
#                                                     is_valid=False, description=None,):
#     d = {}
#     d['source'] = source
#     d['destination'] = destination
#     d['is_valid'] = is_valid
#     if description is not None:
#         d['description'] = description   
#     if owner is not None:
#         d['owner'] = owner    
#     return d

# def generate_refinement_pattern_dictionary(pattern, replacement, order=9999, owner=None, 
#                                                 is_valid=False, description=None):
#     d = {}
#     d['pattern'] = pattern
#     d['replacement'] = replacement
#     d['order'] = order
#     d['is_valid'] = is_valid
#     if description is not None:
#         d['description'] = description   
#     if owner is not None:
#         d['owner'] = owner    
#     return d

# tags_tuple = ('V', 'E', 'N', 'K', 'O', 'N', 'C', 'D')
#     tokens = [
# #Ø¨Ø±ÛŒØ¯_V     Ø¨Ù‡_E     Ø±Ø³ØªÙˆØ±Ø§Ù†_N     https://chilivery.com/tehran/restaurantÛ±/_K     ._O     ØºØ°Ø§_N     Ø´_C     Ø®ÛŒÙ„ÛŒ_D     Ø¹Ø§Ù„ÛŒ_A     Ù‡_C     ğŸ˜ğŸ˜ğŸ˜_X             
#         generate_tagged_token_dictionary('Ø¨Ø±ÛŒØ¯', generate_tag_dictionary(name='A')),
#         generate_tagged_token_dictionary('Ø¨Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø±Ø³ØªÙˆØ±Ø§Ù†', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('https://chilivery.com/tehran/restaurantÛ±/', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('ØºØ°Ø§', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø´', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø®ÛŒÙ„ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¹Ø§Ù„ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('ğŸ˜ğŸ˜ğŸ˜', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# # Ø§Ø²â€ŒÙ†Ø¸Ø±_E     Ù‚ÛŒÙ…Øª_N     Ù…ÛŒØ§Ø±Ø²Ù‡_N     Ú©_C     Ø¨Ø±ÛŒ_V     ÙˆÙ„ÛŒ_J     ÙˆØ§Ù‚Ø¹Ø§_D     Ø®Ù_N     Ø³Øª_C                 
#         generate_tagged_token_dictionary('Ø§Ø²â€ŒÙ†Ø¸Ø±', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù‚ÛŒÙ…Øª', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…ÛŒØ§Ø±Ø²Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ú©', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ø±ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('ÙˆÙ„ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('ÙˆØ§Ù‚Ø¹Ø§', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø®Ù', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø³Øª', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# # Ù…Ø«_N     Ø§ÛŒÙ†_T     Ø¬Ø§ÛŒ_N     Ø¯ÛŒÚ¯Ù‡â€ŒØ§ÛŒ_N     Ù†Ø¯ÛŒØ¯Ù…_V     Ù…Ø´ØªØ±ÛŒØ§Ø´_N     Ø§Ù†Ù‚Ø¯Ø±_D     Ø²ÛŒØ§Ø¯_A     Ø¨Ø§Ø´Ù‡_V     ._O                 
#         generate_tagged_token_dictionary('Ù…Ø«', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§ÛŒÙ†', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¬Ø§ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¯ÛŒÚ¯Ù‡â€ŒØ§ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù†Ø¯ÛŒØ¯Ù…', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…Ø´ØªØ±ÛŒØ§Ø´', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§Ù†Ù‚Ø¯Ø±', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø²ÛŒØ§Ø¯', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ø§Ø´Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# # Ú©ÛŒÙÛŒØª_N  Ù¾ÛŒØªØ²Ø§Ù‡Ø§_N  Ø´_C  Ø§ØµÙ„Ù†_N  Ø®ÙˆØ¨_A  Ù†ÛŒØ³Øª_V  â€¦_O  Ø§Ø²_E  Ø´Ù„ÙˆØºÛŒ_N  Ù…ÛŒØ¯ÙˆÙ†_N  ØªØ¬Ø±ÛŒØ´_N  Ø¯Ø§Ø±Ù‡_V  Ù¾ÙˆÙ„_N  Ø¯Ø±_E  Ù…ÛŒØ§Ø±Ù‡_V  ._O     
#         generate_tagged_token_dictionary('Ú©ÛŒÙÛŒØª', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù¾ÛŒØªØ²Ø§Ù‡Ø§', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø´', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§ØµÙ„Ù†', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø®ÙˆØ¨', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù†ÛŒØ³Øª', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('â€¦', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§Ø²', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø´Ù„ÙˆØºÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…ÛŒØ¯ÙˆÙ†', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('ØªØ¬Ø±ÛŒØ´', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¯Ø§Ø±Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù¾ÙˆÙ„', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¯Ø±Ù…ÛŒØ§Ø±Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# # Ø¨Ø§Ø²_A  Ù…_C  Ø§Ø²_E  Ø´_C  Ø®Ø±ÛŒØ¯_N  Ù…ÛŒâ€ŒÚ©Ù†Ù…_V  ØŒ_O  Ø¨Ù‡ØªØ±ÛŒÙ†_A  Ù…Ù†Ø·Ù‚_N  Ø³Øª_C  ._O  
#         generate_tagged_token_dictionary('Ø¨Ø§Ø²', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§Ø²', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø´', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø®Ø±ÛŒØ¯', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…ÛŒâ€ŒÚ©Ù†Ù…', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ù‡ØªØ±ÛŒÙ†', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…Ù†Ø·Ù‚', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø³Øª', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# #Ø§Ú¯Ù‡_J  ÛŒÙ‡_U  Ø¨Ø§Ø±_N  Ø¨Ø±ÛŒ_V  Ø§ÙˆÙ†Ø¬Ø§_D  Ù…Ø´ØªØ±ÛŒ_N  Ø´_C  Ù…ÛŒØ´ÛŒ_A  ._O  
#         generate_tagged_token_dictionary('Ø§Ú¯Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('ÛŒÙ‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ø§Ø±', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ø±ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§ÙˆÙ†Ø¬Ø§', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…Ø´ØªØ±ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø´', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…ÛŒØ´ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# #Ø¨Ø±Ø§ÛŒ_E  Ø§Ø±Ø³Ø§Ù„_N  Ø±Ø²ÙˆÙ…Ù‡_N  Ø¨Ù‡_E  Ø¢Ø¯Ø±Ø³_N  job@gmail.com_M  Ù…ÛŒÙ„_N  Ø¨Ø²Ù†ÛŒØ¯_V  ._O           
#         generate_tagged_token_dictionary('Ø¨Ø±Ø§ÛŒ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø§Ø±Ø³Ø§Ù„', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø±Ø²ÙˆÙ…Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¢Ø¯Ø±Ø³', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('job@gmail.com', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù…ÛŒÙ„', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø¨Ø²Ù†ÛŒØ¯', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('.', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# #Ú©ØªØ§Ø¨_N  Ù‡_C  Ø±Ùˆ_P  ØªÙˆ_N  ÙØ±Ù‡Ù†Ú¯â€ŒØ³Ø±Ø§_A  Ù¾ÛŒØ¯Ø§_A  Ø´_C  Ú©Ø±Ø¯Ù…_V  ._O           
#         generate_tagged_token_dictionary('Ú©ØªØ§Ø¨', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ù‡', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('Ø±Ùˆ', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# #Ø¯Ø±â€ŒØµÙˆØ±Øª_E  Ù…Ø´Ú©Ù„_N  Ø¨Ù‡_E  Ø¢ÛŒØ¯ÛŒ_N  @bitianist_S  Ù¾ÛŒØ§Ù…_N  Ø¨Ø¯Ù‡ÛŒØ¯_V  ._O  
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# #Ø­Ø§Ø¶Ø±_A  Ù…_C  Ø´Ø±Ø·_N  Ø¨Ù†Ø¯_N  Ù…_C  Ù‡Ù…ÙˆÙ†_Z  Ùˆ_J  ÙˆØ±Ø¯Ø§Ø´ØªÙ†_V  ØªØ²ÛŒÛŒÙ†_N  Ú©Ø±Ø¯Ù†_V  Ùˆ_J  Ø¨Ø±Ø§ÛŒ_E  Ù…Ø§_Z  Ø§ÙˆÙˆØ±Ø¯Ù†_V  !_O              
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('', generate_tag_dictionary(name='N')),
#         generate_tagged_token_dictionary('\\n', generate_tag_dictionary(name='N')),
# #Ø¢Ø®Ù‡_D  ÙˆØ§Ù‚Ø¹Ø§_D  Ø¯Ø±Ø³Øª_A  Ù‡_C  Ø§ÛŒÙ†Ú©Ø§Ø±_N  ØŸ_O  Ú†Ø±Ø§_D  Ø®ÙˆØ¨_D  Ø¯Ø±Ø³Øª_A  Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†_V  ØŸ_O            
#     ]